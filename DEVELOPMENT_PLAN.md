# 今後の開発プラン

ここでは本プロジェクトの改善点や追加したい機能について、現時点のアイデアをまとめます。

## 1. 学習アルゴリズムの拡充

- **モンテカルロ木探索(MCTS)との組み合わせ**
  - PolicyAgent の方策を初期値として使用し、MCTS で探索を深める。
  - より強力なプレイヤーを目指す。
- **AlphaZero 風の学習**
  - 価値ネットワークと方策ネットワークを統合し、自己対戦からデータ収集。
  - 多数の GPU を用いた並列学習も検討。

## 2. 環境・ルールのカスタマイズ性向上

- 盤面サイズや連勝条件を柔軟に変えられるようパラメータ化を進める。
- ハンディキャップ(先手固定や置き石)のオプション追加。
- GUI 上で設定を変更できるようにする。

## 3. GUI の改善

- Pygame での表示をより見やすくし、アニメーションや操作説明を強化。
- キーボード入力だけでなくマウス操作にも対応する。
- 対戦結果の履歴を保存し、あとで再生できる機能を検討。

## 4. モデル管理・評価基盤

- 複数バージョンのモデルを一元管理できるようディレクトリ構成を整理。
- `evaluate_models.py` の結果を自動集計しグラフ化する仕組みを追加。
- 総当たり戦 (`round_robin.py`) の結果を Web 上で閲覧できるようにする。

## 5. コード品質向上

- テスト用スクリプトを用意し、基本的な機能が動作するか確認できるようにする。
- ドキュメントを充実させ、各関数の使い方や引数の意味を明確にする。
- 型ヒントの導入範囲を広げ、エディタでの補完をしやすくする。

## 6. コミュニティとの連携

- 学習済みモデルやプレイ動画を公開し、フィードバックを集める。
- Issue や Pull Request を通じて他の開発者からの貢献を受け付ける体制を整える。
- 大会やコンテストがあれば参加を検討する。

以上、現段階で想定している主な開発項目です。状況に応じて柔軟に方針を見直していきます。
